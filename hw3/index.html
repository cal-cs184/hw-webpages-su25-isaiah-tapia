<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;

			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body style="background-color:rgb(24,24,24);color: #ebdbb2;line-height: 1.5;">
		<div class="container">
		<h1>CS184 Summer 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Isaiah Tapia & 	Meghai Choudhury </div>

		<br>

		Link to webpage: <a href="https://cal-cs184.github.io/hw-webpages-su25-isaiah-tapia/hw3/index.html"> web_page_link </a>
			<p></p>
		Link to GitHub repository: <a href="https://github.com/cal-cs184/hw-pathtracer-updated-0__0"> git_hub_repo </a>
		
		<figure>
<!--			<img src="title.png" alt="Cornell Boxes with Bunnies: direct vs indirect" style="width:70%"/>-->
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		In this homework, we implemented a physically-based path tracer that supports both direct and global illumination.
		We started by generating camera rays and computing intersections with scene geometry, form these principal components
		we built the rest of our pipeline. Afterward, we implemented and then used a bounding volume hierarchy (BVH)
		to speed up our ray intersection checks.
		<br> <br>
		We explored two methods for direct lighting: uniform hemisphere sampling and importance sampling. The latter gave much cleaner results with fewer samples, especially around soft shadows. Adding global illumination with recursive ray bounces brought scenes to life with more realistic lighting, and we used Russian Roulette to balance quality and efficiency. Finally, adaptive sampling helped us reduce unnecessary work by stopping early when pixel values had already converged.
		<br> <br>
		Overall, we gained a deeper understanding of how light transport works and how each component of the pipeline contributes to producing realistic images.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h3>Ray Generation and Primitive Intersection</h3>
		<p>
			Our code generates a ray by calculating the direction of our ray using creating a Vector3D with both fov's (h and w)
			\[x= \left(2 \cdot x - 1\right) \cdot \tan\left(\frac{\text{radians}\left({\text{hFov}}\right)}{2}\right) \]
			\[y= \left(2 \cdot y - 1\right) \cdot \tan\left(\frac{\text{radians}\left({\text{vFov}}\right)}{2}\right) \]
			\[z = -1 \]
			We then normalize this vector and change it from camera
			space to the world space. We then make a ray object where our origin is our camera position and our direction is
			the normalized world space direction vector from earlier. We then set our rays min_t and max_t to nClip and fClip
			respectively. Finally return the ray.
		</p>

		<h3>Triangle Intersection Algorithm</h3>
		<p>
			For our implementation we used the Moller Trumbore intersection algorithm to determine whether or not a ray
			intersected a triangle. In essence our implementation checked to see if our ray is intersecting the triangle,
			the core components used to make this check was barycentric coordinates. We first create a point from the ray
			onto our plane, then we leverage the fact that we are keeping track of each triangles' points to then use
			barycentric coordinates to determine if our 'hitpoint' is within the triangle.
		</p>

		<p>
			We also implemented a sphere intersection method that had a similar goal of determining the existence of an intersection
		</p>


			<div style="display: flex; justify-content: space-between; align-items: flex-start;">
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/teapot.png" style="width: 120%;" />
					<figcaption>Teapot render with debug shading</figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/CBspheres.png" style="width: 120%;" />
					<figcaption>Spheres </figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/CBgems.png" style="width: 120%;" />
					<figcaption>gems</figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/bunny_debug.png" style="width: 100%;" />
					<figcaption>bunny</figcaption>
				</figure>
			</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
			<p>
				Our bounding volume hierarchy algorithm first traverses through all the bounding boxes of the node that is passed
				in as a parameter. We then compute the difference between the start and end positions of the node iterator,
				this will give us the distance between the two values.
			</p>

			<p>
				After that we make our “is leaf” check, this acts as our recursive base case that we’ll be relying on down the
				line. If we do have a leaf node then we know that its left and right pointers should be set to null and we
				return the node. Ending the bounding box creation. Allowing for quicker ray tracing.
			</p>

			<p>
				If our node is in fact not a leaf, then we compute the bounding box of the centroid expanding on each while
				we iterate through all the centroid.
			</p>

			<pre>
	  BBox centroid_bounds ;
	  for (auto i = start; i != end; i++) {
		Vector3D c = (*i)->get_bbox().centroid();
		centroid_bounds.expand(c);
	  }
			</pre>
			<p>
				With this new bonding box we want to use it to compute the new axis we ought to be splitting on.
				This will be determined by the difference between the centroid bounds' max and min values with respect
				to each one's x,y,z. Afterwards we set our axis to correspond to whichever axis distance is the greatest.
				We use this axis to get our midpoint heuristic. And then sorting our start and end points correspondingly.
			</p>
			<pre>
//calculate split point(midpoint along the selected acis)
double mid = centroid_bounds.centroid()[axis];
//(centroid_bounds.max[axis] + centroid_bounds.min[axis]) / 2.0;
// partition primitives based on their centroid position relative to split point
auto partition_iter = std::partition(start, end, [axis, mid](Primitive* p) {
	return p->get_bbox().centroid()[axis] < mid  ;
});
			</pre>
			<p>
				Falling back on partitioning in the middle if all else fails.

				Finally, since we aren't at a leaf node we recursively construct the node's left and right sub-trees
			</p>
			<pre>
	node->l = construct_bvh(start, partition_iter, max_leaf_size);
	node->r = construct_bvh(partition_iter, end, max_leaf_size);
			</pre>

			<div style="display: flex; justify-content: space-between; align-items: flex-start;">
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/blob.png" style="width: 120%;" />
					<figcaption>blob rendered with BBoxes</figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/dragon.png" style="width: 120%;" />
					<figcaption>Dragon rendered with BBoxes </figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/lucy.png" style="width: 120%;" />
					<figcaption>Lucy rendered with BBoxes</figcaption>
				</figure>
				<figure style="margin: 0 10px; text-align: center;">
					<img src="new_images/planck.png" style="width: 120%;" />
					<figcaption>Max Planck rendered with BBoxes</figcaption>
				</figure>
			</div>
		<h2>Part 3: Direct Illumination</h2>
		The two direct lighting functions, <code> estimate_direct_lighting_hemisphere</code> and <code>estimate_direct_lighting_importance</code> both try to figure out how much light hits a point on a surface, but go about it in different ways.
		<br>		<br>

		<div style="display: flex; justify-content: space-between; align-items: flex-start;">
		<figure style="margin: 0 10px; text-align: center;">
			<img src="3.3.png" style="width: 100%;" />
			<figcaption>Direct Lighting with Uniform Hemisphere Sampling: Bunny</figcaption>
		</figure>
		<figure style="margin: 0 10px; text-align: center;">
			<img src="spheresH.png" style="width: 100%;" />
			<figcaption>Direct Lighting with Uniform Hemisphere Sampling: Spheres</figcaption>
		</figure>
		</div>

		<p>
		The <code>estimate_direct_lighting_hemisphere</code> function samples directions randomly over the hemisphere above the surface.
		For each sample direction, it traces a ray from the intersection point to see if it hits a light source. If it does, it calculates
		how much light is coming from that direction using the BSDF, the emitted light from the hit surface, the cosine of the angle
		between the direction and normal, and a constant pdf, averaging the contributions to estimate the direct lighting.
		</p>

			<p>\[ \begin{align*}
				L_{\text{sample}} &= \frac{f_r(\omega_{\text{out}}, \omega_{\text{in}}) L_i \cos\Theta}{\text{pdf}} \

				\end{align*} \]
			</p>
			<p>
				\[ \text{pdf} = \frac{1}{2\pi} \]
			</p>
<!--		<p style="font-family: monospace; font-size: 1.1em;">-->
<!--		L<sub>sample</sub> = (f<sub>r</sub>(w<sub>out</sub>, w<sub>in</sub>) × L<sub>i</sub> × cosΘ) / pdf<br>-->
<!--		pdf = 1 / 2π-->
<!--		</p>-->

		<div style="display: flex; justify-content: space-between; align-items: flex-start;">
		<figure style="margin: 0 10px; text-align: center;">
			<img src="3.4.png" style="width: 100%;" />
			<figcaption>Direct Lighting with Importance Sampling: Bunny</figcaption>
		</figure>
		<figure style="margin: 0 10px; text-align: center;">
			<img src="spheres.png" style="width: 100%;" />
			<figcaption>Direct Lighting with Importance Sampling: Spheres</figcaption>
		</figure>
		</div>

		<p>
		The <code>estimate_direct_lighting_importance</code> function samples directly from the lights on the scene.
		Instead of sampling random directions, it asks each light where the light is coming from and how much of it
		reaches the surface. For every sample, it checks whether the light is visible by shooting a shadow ray.
		If the light isn't blocked, it calculates the BSDF value and scales it by the incoming light, cosine of the angle,
		and the sample's pdf (same as in <code>estimate_direct_lighting_hemisphere</code>).
		</p>

<!--		<p style="font-family: monospace; font-size: 1.1em;">-->
<!--		L<sub>sample</sub> = (f<sub>r</sub>(w<sub>out</sub>, w<sub>in</sub>) × L<sub>i</sub> × cosΘ) / pdf<br>-->
<!--		pdf = 1 / 2π-->
<!--		</p>-->

			<p>\[ \begin{align*}
				L_{\text{sample}} &= \frac{f_r(\omega_{\text{out}}, \omega_{\text{in}}) L_i \cos\Theta}{\text{pdf}} \

				\end{align*} \]
			</p>
			<p>
				\[ \text{pdf} = \frac{1}{2\pi} \]
			</p>

		<p>
		Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with
		1, 4, 16, and 64 light rays and with 1 sample per pixel using light sampling:
		</p>

		<!-- Image row -->
		<div style="display: flex; justify-content: space-between; text-align: center;">
		<figure style="width: 23%; margin: 0 1%;">
			<img src="3-1.png" style="width: 100%;" alt="Image 3-1">
		</figure>
		<figure style="width: 23%; margin: 0 1%;">
			<img src="3-2.png" style="width: 100%;" alt="Image 3-2">
		</figure>
		<figure style="width: 23%; margin: 0 1%;">
			<img src="3-3.png" style="width: 100%;" alt="Image 3-3">
		</figure>
		<figure style="width: 23%; margin: 0 1%;">
			<img src="3-4.png" style="width: 100%;" alt="Image 3-4">
		</figure>
		</div>

		<!-- Captions for each image -->
		<div style="display: flex; justify-content: space-between; text-align: center; margin-top: 10px;">
		<div style="width: 23%; margin: 0 1%;">1 light ray: lots of noise in soft shadows</div>
		<div style="width: 23%; margin: 0 1%;">4 light rays: noise slightly decreases</div>
		<div style="width: 23%; margin: 0 1%;">16 light rays: noise is significantly reduced in soft shadows</div>
		<div style="width: 23%; margin: 0 1%;">64 light rays: almost no noise</div>
		</div>

		<p>
			Uniform hemisphere sampling tends to produce noisier and less efficient results compared to lighting (importance) sampling, especially in scenes with small or few light sources. Since it samples directions blindly across the entire hemisphere, many rays miss the lights completely and contribute nothing to the final image, which leads to a grainy appearance unless a very high number of samples is used. Lighting sampling, on the other hand, focuses sample directions toward actual light sources, so more of the samples contribute meaningful lighting information. This leads to cleaner, more accurate results with fewer samples, especially when dealing with area lights or scenes where indirect paths to the lights are less likely to be randomly hit.
		</p>
			<h2>Part 4: Global Illumination</h2>
		 Images rendered with global (direct and indirect) illumination (1024 samples per pixel, Number of samples per area of light is 1, Depth per ray of light is 5)
		
		 <div style="display: flex; justify-content: space-between; gap: 20px;">
		<!-- Left: Bunny -->
		<div style="width: 48%; text-align: center;">
			<p style="margin-bottom: 10px;">
			1024 global illumination bunny
			</p>
			<img src="new_images/1024_global_indirect.png" style="width: 100%; border: 2px solid black;" >
		</div>

		<!-- Right: Spheres -->
		<div style="width: 48%; text-align: center;">
			<p style="margin-bottom: 10px;">
			1024 global illumination spheres
			</p>
			<img src="new_images/1024_global_indirect_spheres.png" style="width: 100%; border: 2px solid black;">
		</div>
		</div>

		<p> Comparison of rendered views first with only direct illumination, then only indirect illumination (1024 samples per pixel)</p>
		<div style="display: flex; justify-content: space-between; gap: 20px;">
		<!-- Left: Bunny -->
		<div style="width: 48%; text-align: center;">
			<p style="margin-bottom: 10px;">
			Only direct illumination
			</p>
			<img src="new_images/1024_global_direct.png" style="width: 100%; border: 2px solid black;" >
		</div>

		<!-- Right: Spheres -->
		<div style="width: 48%; text-align: center;">
			<p style="margin-bottom: 10px;">
			Only indirect illumination
			</p>
			<img src="new_images/1024_global_both_bunny.png" style="width: 100%; border: 2px solid black;">
		</div>
		</div>
		
		<br> <br><br>


		<style>
		table {
			width: 100%;
			table-layout: fixed;
			border-collapse: collapse;
		}
		th, td {
			width: 14.28%;
			text-align: center;
			vertical-align: middle;
			border: 1px solid #444;
			font-family: sans-serif;
		}
		img {
			width: 100%;
			height: auto;
			display: block;
		}
		</style>

		<table>
		<tr>
			<th></th>
			<th>max_ray_<br>depth = 0</th>
			<th>max_ray_<br>depth = 1</th>
			<th>max_ray_<br>depth = 2</th>
			<th>max_ray_<br>depth = 3</th>
			<th>max_ray_<br>depth = 4</th>
			<th>max_ray_<br>depth = 5</th>
		</tr>
		<tr>
			<td>isAccumBounces = false</td>
			<td><img src="bunny0.png" alt="bunny0"></td>
			<td><img src="bunny1.png" alt="bunny1"></td>
			<td><img src="bunny2.png" alt="bunny2"></td>
			<td><img src="bunny3.png" alt="bunny3"></td>
			<td><img src="bunny4.png" alt="bunny4"></td>
			<td><img src="bunny5.png" alt="bunny5"></td>
		</tr>
		<tr>
			<td>isAccumBounces = true</td>
			<td><img src="bunny0a.png" alt="bunny0a"></td>
			<td><img src="bunny1a.png" alt="bunny1a"></td>
			<td><img src="bunny2a.png" alt="bunny2a"></td>
			<td><img src="bunny3a.png" alt="bunny3a"></td>
			<td><img src="bunny4a.png" alt="bunny4a"></td>
			<td><img src="bunny5a.png" alt="bunny5a"></td>
		</tr>
		</table>

		<p> 
		When rendering CBbunny.dae with <code>max_ray_depth  </code>set to 0 through 5 and isAccumBounces set to false, each image shows only the m-th bounce of light. At the second bounce count, light has already reflected off one surface before bouncing again and reaching the camera, adding subtle details like soft shadows, indirect illumination in corners, etc. At the third bounce, the image becomes noticeably darker, because after two reflections, the contribution of light of the third bounce is weaker. Compared to rasterization, which doesn't simulate any of this indirect light, even the darker third bounce adds realism by capturing subtle lighting behavior that makes the scene feel more natural.</p>

		<p> Russian Roulette Rendering: For CBbunny.dae, output the Russian Roulette rendering with <code> max_ray_depth</code> set to 0, 1, 2, 3, 4, and 100(the -m flag). Use 1024 samples per pixel:
		</p>

		<style>
		table {
			width: 100%;
			table-layout: fixed;
			border-collapse: collapse;
		}
		th, td {
			width: 14.28%;
			text-align: center;
			vertical-align: middle;
			border: 1px solid #444;
			font-family: sans-serif;
		}
		img {
			width: 100%;
			height: auto;
			display: block;
		}
		</style>

		<table>
		<tr>
			<th>max_ray_<br>depth = 0</th>
			<th>max_ray_<br>depth = 1</th>
			<th>max_ray_<br>depth = 2</th>
			<th>max_ray_<br>depth = 3</th>
			<th>max_ray_<br>depth = 4</th>
			<th>max_ray_<br>depth = 100</th>
		</tr>
		<tr>
			<td><img src="bunny0r.png" alt="bunny0r"></td>
			<td><img src="bunny1r.png" alt="bunny1r"></td>
			<td><img src="bunny2r.png" alt="bunny2r"></td>
			<td><img src="bunny3r.png" alt="bunny3r"></td>
			<td><img src="bunny4r.png" alt="bunny4r"></td>
			<td><img src="bunny100r.png" alt="bunny100r"></td>
		</tr>
		</table>
		
		<p> Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays:</p>
		<style>
		table {
			width: 100%;
			table-layout: fixed;
			border-collapse: collapse;
		}
		th, td {
			width: 14.28%; /* 100 / 7 columns */
			text-align: center;
			vertical-align: middle;
			border: 1px solid #444;
			font-family: sans-serif;
		}
		img {
			width: 100%;
			height: auto;
			display: block;
		}
		</style>

		<table>
		<tr>
			<th>samples<br>per pixel = 1</th>
			<th>samples<br>per pixel = 2</th>
			<th>samples<br>per pixel = 4</th>
			<th>samples<br>per pixel = 8</th>
			<th>samples<br>per pixel = 16</th>
			<th>samples<br>per pixel = 64</th>
			<th>samples<br>per pixel = 1024</th>
		</tr>
		<tr>
			<td><img src="dragon1.png" alt="dragon1"></td>
			<td><img src="dragon2.png" alt="dragon2"></td>
			<td><img src="dragon4.png" alt="dragon4"></td>
			<td><img src="dragon8.png" alt="dragon8"></td>
			<td><img src="dragon16.png" alt="dragon16"></td>
			<td><img src="dragon64.png" alt="dragon64"></td>
			<td><img src="dragon1024.png" alt="dragon1024"></td>
		</tr>
		</table>

		<p> The <code> at_least_one_bounce_radiance </code> function estimates indirect lighting by recursively bouncing rays through the scene. It first adds direct lighting if needed, then samples the BSDF to get a new direction. A new ray is traced from the intersection point, and if it hits something, the function calls itself again to get the incoming radiance. The result is scaled using the BSDF, cosine term, and PDF. If the ray misses and there's an environment light, it samples that instead. Russian roulette is used to probabilistically end paths and keep the computation efficient.</p>
			<br></br>
		<h2>Part 5: Adaptive Sampling</h2>
			<p>
				The purpose of adaptive sampling is that it allows us to terminate sampling earlier using statistical methods.
				For pixels that converge faster, we check to see if they have remained the same, we perform this check
				when we have iterated over a pixel a predetermined amount of times. While iterating we should be keeping
				track of the sum of the illumination of the rays as well as the summation of the illumination values squared.
			</p>
			<div style="display: flex;
  						justify-content: center;
						align-items: center;">
				\(
				s_1 = \sum_{k=1}^{n} x_k
				,\quad
				s_2 = \sum_{k=1}^{n} x_k^2
				\)
			</div>
			<p>We use these values to then compute the mean and the variance of our current number of samples</p>
			\[
			\mu = \frac{s_1}{n}
			\]
			\[
			\sigma^2 = \frac{1}{n - 1} \cdot \left( s_2 - \frac{s_1^2}{n} \right)
			\]

			<p>
				If we find that our I value is less than the maxTolerance multiplied by the mean of our samples, we can
				assume, with a 95% confidence, that this pixel's value has converged and we can exit our current loop.
			</p>


			<p> Our confidence interval, 95%, is computed by calculating 1.96 times the std over the sqrt of the numer of samples
			</p>

			\[
			I  = 1.96 \cdot \frac{\sigma}{\sqrt{n}}
			\]

			<p>Heres is a table of our 2048 renders called using <code style="color: yellow">-t 8 -s 2048 -a 32 0.05 -l 1 -m 5 -r 480 360
			</code></p>

			<div style="display: flex; justify-content: space-between; align-items: flex-start;">
				<lu>
					<figure style="margin: 0 10px; text-align: center;">
						<img src="new_images/spheres_samp.png" style="width: 100%;" />
						<figcaption>blob rendered with BBoxes</figcaption>
					</figure>
					<figure style="margin: 0 10px; text-align: center;">
						<img src="new_images/spheres_samp_rate.png" style="width: 100%;" />
						<figcaption>Dragon rendered with BBoxes </figcaption>
					</figure>
				</lu>

				<lu>
					<figure style="margin: 0 10px; text-align: center;">
						<img src="new_images/bunny_new_one.png" style="width: 100%;" />
						<figcaption>Lucy rendered with BBoxes</figcaption>
					</figure>
					<figure style="margin: 0 10px; text-align: center;">
						<img src="new_images/bunny_new_one_rate.png" style="width: 100%;" />
						<figcaption>Max Planck rendered with BBoxes</figcaption>
					</figure>
				</lu>
			</div>


			<h2>AI Usage</h2>

		</div>
	</body>
</html>